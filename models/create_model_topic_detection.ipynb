{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "presidential-flexibility",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8a56e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T20:54:24.214346Z",
     "start_time": "2022-11-21T20:54:22.217568Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/belr/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/belr/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/belr/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Error loading average_perceptron_tagger: Package\n",
      "[nltk_data]     'average_perceptron_tagger' not found in index\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk import pos_tag\n",
    "from nltk import sent_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import stopwords as sw\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('average_perceptron_tagger')\n",
    "\n",
    "stopwords = sw.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-while",
   "metadata": {},
   "source": [
    "## MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "classified-intranet",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T20:54:29.314438Z",
     "start_time": "2022-11-21T20:54:29.145262Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id_': {'v': 2, 'key': [('_id', 1)]},\n",
       " 'Id_1': {'v': 2, 'key': [('Id', 1)]},\n",
       " 'PostTypeId_1': {'v': 2, 'key': [('PostTypeId', 1)]},\n",
       " 'Topic_1': {'v': 2, 'key': [('Topic', 1)]},\n",
       " 'Score_1': {'v': 2, 'key': [('Score', 1)]},\n",
       " 'ParentId_1': {'v': 2, 'key': [('ParentId', 1)]},\n",
       " 'AnswerCount_1': {'v': 2, 'key': [('AnswerCount', 1)]},\n",
       " 'text_search': {'v': 2,\n",
       "  'key': [('_fts', 'text'), ('_ftsx', 1)],\n",
       "  'weights': SON([('Body', 1), ('Title', 2)]),\n",
       "  'default_language': 'english',\n",
       "  'language_override': 'language',\n",
       "  'textIndexVersion': 3}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "\n",
    "# uri = \"mongodb://%s:%s@%s\" % ('root', 'belr', 'localhost')\n",
    "uri = \"mongodb://root:belr@localhost:27017\"\n",
    "client = MongoClient(uri)\n",
    "\n",
    "db = client[\"DiscordSmartbotDB\"]\n",
    "collection = db['Quest_Rep']\n",
    "collection.index_information()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "documentary-minister",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T20:54:34.648432Z",
     "start_time": "2022-11-21T20:54:34.607493Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['astronomy', 'datascience', 'earthscience', 'engineering', 'general', 'space']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics= collection.distinct('Topic')\n",
    "topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "living-hanging",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T20:54:38.554879Z",
     "start_time": "2022-11-21T20:54:38.546318Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_topics = len(topics)\n",
    "nb_topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "severe-finland",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T20:54:43.120655Z",
     "start_time": "2022-11-21T20:54:43.106162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# topics = ['astronomy', 'bicycles', 'earthscience', 'engineering',\n",
    "#           'general', 'space', 'stellar']\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(topics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lovely-clause",
   "metadata": {},
   "source": [
    "## Datasets (corpus & target) generation\n",
    "\n",
    "- Query MongoDB to create a list of questions (corpus)\n",
    "- Select n questions for each topic => max_by_topic (if topic exists ...)\n",
    "- Select all available questions in the topic (if not)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "monetary-portal",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T20:54:50.856859Z",
     "start_time": "2022-11-21T20:54:47.824338Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = []\n",
    "target = []\n",
    "topic_max_questions = 5000\n",
    "\n",
    "for topic in topics:\n",
    "\n",
    "    query = {\"Topic\": topic, \"PostTypeId\": \"1\"}\n",
    "\n",
    "    if collection.count_documents(query) >= topic_max_questions:\n",
    "\n",
    "        questions = collection.find(query).sort([('Score', -1)]).limit(topic_max_questions)\n",
    "\n",
    "        corpus.extend([question.get(\"Title\") for question in questions])\n",
    "        target.extend([topic] * topic_max_questions)\n",
    "\n",
    "    else:\n",
    "        length = collection.count_documents(query)\n",
    "\n",
    "        questions = collection.find(query).sort([('Score', -1)]).limit(length)\n",
    "\n",
    "        corpus.extend([question.get(\"Title\") for question in questions])\n",
    "        target.extend([topic] * length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "operational-pathology",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T20:54:54.650808Z",
     "start_time": "2022-11-21T20:54:54.553755Z"
    }
   },
   "outputs": [],
   "source": [
    "corpus = np.array(corpus)\n",
    "target = np.array(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "perfect-string",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T20:55:01.816827Z",
     "start_time": "2022-11-21T20:55:00.823650Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astronomy \n",
      "\t\tsize = 9995 \n",
      "\n",
      "datascience \n",
      "\t\tsize = 26279 \n",
      "\n",
      "earthscience \n",
      "\t\tsize = 5393 \n",
      "\n",
      "engineering \n",
      "\t\tsize = 10324 \n",
      "\n",
      "general \n",
      "\t\tsize = 2619 \n",
      "\n",
      "space \n",
      "\t\tsize = 14295 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Topics size\n",
    "for topic in topics:\n",
    "    query = {\"Topic\": topic, \"PostTypeId\": \"1\"}\n",
    "    topic_size = collection.count_documents(query)\n",
    "    print(topic, \"\\n\\t\\tsize =\", topic_size, \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "prospective-slope",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T20:55:08.063672Z",
     "start_time": "2022-11-21T20:55:08.025554Z"
    }
   },
   "outputs": [],
   "source": [
    "# encode targets (str) to numerical (int)\n",
    "y = label_encoder.transform(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "separated-first",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T20:55:12.545940Z",
     "start_time": "2022-11-21T20:55:11.755308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27619, 17721)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=stopwords)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suited-collectible",
   "metadata": {},
   "source": [
    "## Test some models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "wired-immune",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T20:55:18.569339Z",
     "start_time": "2022-11-21T20:55:18.554166Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "def run_pipes(pipes, splits, corpus, target, test_size=0.2, seed=42):  \n",
    "    res = defaultdict(list)\n",
    "    spliter = ShuffleSplit(n_splits=splits, test_size=test_size, random_state=seed)\n",
    "\n",
    "    for idx_train, idx_test in spliter.split(corpus, target):\n",
    "\n",
    "        for pipe in pipes:\n",
    "\n",
    "            # Name of the model\n",
    "            name = \"-\".join([x[0] for x in pipe.steps])\n",
    "            \n",
    "            # Split datasets\n",
    "            X_train = corpus[idx_train]\n",
    "            X_test = corpus[idx_test]\n",
    "            y_train = target[idx_train]\n",
    "            y_test = target[idx_test]\n",
    "            \n",
    "            # Train\n",
    "            start = time()\n",
    "            pipe.fit(X_train, y_train)\n",
    "            fit_time = time() - start\n",
    "            \n",
    "            # Test & save results\n",
    "            y = pipe.predict(X_test)\n",
    "            res[name].append([\n",
    "                fit_time,\n",
    "                f1_score(y_test, y, average='micro'),\n",
    "                f1_score(y_test, y, average='macro'),\n",
    "                f1_score(y_test, y, average='weighted'),           \n",
    "            ])\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "valuable-tradition",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T20:55:32.010400Z",
     "start_time": "2022-11-21T20:55:30.996278Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def print_table(res):\n",
    "    # Compute mean & std\n",
    "    final = {}\n",
    "    for model in res:\n",
    "        arr = np.array(res[model])\n",
    "        final[model] = {\n",
    "            \"time (s)\" : arr[:, 0].mean().round(2),\n",
    "            \"f1_av_micro\": [arr[:,1].mean().round(3), arr[:,1].std().round(3)],\n",
    "            \"f1_av_macro\": [arr[:,2].mean().round(3), arr[:,2].std().round(3)],\n",
    "            \"f1_av_weighted\": [arr[:,3].mean().round(3), arr[:,3].std().round(3)],\n",
    "        }\n",
    "\n",
    "    df = pd.DataFrame.from_dict(final, orient=\"index\").round(3)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acknowledged-toolbox",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T21:58:13.826598Z",
     "start_time": "2022-11-21T20:55:44.653298Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/belr/Python/Projects/discord-smartbot/venv/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/belr/Python/Projects/discord-smartbot/venv/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/belr/Python/Projects/discord-smartbot/venv/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/belr/Python/Projects/discord-smartbot/venv/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "/home/belr/Python/Projects/discord-smartbot/venv/lib/python3.10/site-packages/sklearn/svm/_base.py:1225: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time (s)</th>\n",
       "      <th>f1_av_micro</th>\n",
       "      <th>f1_av_macro</th>\n",
       "      <th>f1_av_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ngram_stop-tfidf-sgd-weight_bal</th>\n",
       "      <td>2.19</td>\n",
       "      <td>[0.857, 0.005]</td>\n",
       "      <td>[0.863, 0.005]</td>\n",
       "      <td>[0.856, 0.005]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram_stop-tfidf-lin_svm-weight_bal</th>\n",
       "      <td>4.05</td>\n",
       "      <td>[0.851, 0.006]</td>\n",
       "      <td>[0.856, 0.006]</td>\n",
       "      <td>[0.85, 0.006]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram_stop-tfidf-lgr-weight_bal</th>\n",
       "      <td>345.15</td>\n",
       "      <td>[0.816, 0.004]</td>\n",
       "      <td>[0.822, 0.004]</td>\n",
       "      <td>[0.816, 0.004]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram_stop-tfidf-dtc-weight_bal</th>\n",
       "      <td>20.85</td>\n",
       "      <td>[0.719, 0.006]</td>\n",
       "      <td>[0.723, 0.006]</td>\n",
       "      <td>[0.716, 0.006]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ngram_stop-tfidf-compl_nb-weight_bal</th>\n",
       "      <td>1.19</td>\n",
       "      <td>[0.848, 0.003]</td>\n",
       "      <td>[0.853, 0.004]</td>\n",
       "      <td>[0.848, 0.003]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      time (s)     f1_av_micro  \\\n",
       "ngram_stop-tfidf-sgd-weight_bal           2.19  [0.857, 0.005]   \n",
       "ngram_stop-tfidf-lin_svm-weight_bal       4.05  [0.851, 0.006]   \n",
       "ngram_stop-tfidf-lgr-weight_bal         345.15  [0.816, 0.004]   \n",
       "ngram_stop-tfidf-dtc-weight_bal          20.85  [0.719, 0.006]   \n",
       "ngram_stop-tfidf-compl_nb-weight_bal      1.19  [0.848, 0.003]   \n",
       "\n",
       "                                         f1_av_macro  f1_av_weighted  \n",
       "ngram_stop-tfidf-sgd-weight_bal       [0.863, 0.005]  [0.856, 0.005]  \n",
       "ngram_stop-tfidf-lin_svm-weight_bal   [0.856, 0.006]   [0.85, 0.006]  \n",
       "ngram_stop-tfidf-lgr-weight_bal       [0.822, 0.004]  [0.816, 0.004]  \n",
       "ngram_stop-tfidf-dtc-weight_bal       [0.723, 0.006]  [0.716, 0.006]  \n",
       "ngram_stop-tfidf-compl_nb-weight_bal  [0.853, 0.004]  [0.848, 0.003]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import (SGDClassifier, LogisticRegression,\n",
    "                                  LogisticRegressionCV)\n",
    "\n",
    "# penalty => ‘l2’, ‘l1’, ‘elasticnet’\n",
    "# loss => ‘hinge’, ‘log’, ‘modified_huber’, ‘squared_hinge’, ‘perceptron’\n",
    "# regression loss => ‘squared_loss’, ‘huber’, ‘epsilon_insensitive’, or ‘squared_epsilon_insensitive’\n",
    "pipe0 = Pipeline([\n",
    "    (\"ngram_stop\", CountVectorizer(\n",
    "        stop_words=stopwords, ngram_range=(1,2), min_df=2\n",
    "    )),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('sgd-weight_bal', SGDClassifier(\n",
    "        max_iter=2000, class_weight='balanced',\n",
    "        penalty='elasticnet', loss='modified_huber'\n",
    "    )),\n",
    "])\n",
    "\n",
    "# penalty{‘l1’, ‘l2’} default='l2'\n",
    "# loss{‘hinge’, ‘squared_hinge’} default='square_hinge'\n",
    "# MUST => penalty='l1', loss = 'squared_hinge', dual=False\n",
    "pipe1 = Pipeline([\n",
    "    (\"ngram_stop\", CountVectorizer(\n",
    "        stop_words=stopwords, ngram_range=(1,2), min_df=2\n",
    "    )),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('lin_svm-weight_bal', LinearSVC(\n",
    "        class_weight='balanced', penalty='l1', loss='squared_hinge', dual=False\n",
    "    )),\n",
    "])\n",
    "\n",
    "# penalty{‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, dualbool, default=False\n",
    "# solver{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’\n",
    "pipe2 = Pipeline([\n",
    "    (\"ngram_stop\", CountVectorizer(\n",
    "        stop_words=stopwords, ngram_range=(1,2), min_df=2\n",
    "    )),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('lgr-weight_bal', LogisticRegression(\n",
    "        max_iter=2000, penalty='l1',\n",
    "        solver='liblinear', class_weight='balanced'\n",
    "    )),\n",
    "])\n",
    "\n",
    "pipe3 = Pipeline([\n",
    "    (\"ngram_stop\", CountVectorizer(\n",
    "        stop_words=stopwords, ngram_range=(1,2), min_df=2\n",
    "    )),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('dtc-weight_bal', DecisionTreeClassifier(class_weight='balanced')),\n",
    "])\n",
    "\n",
    "pipe4 = Pipeline([\n",
    "    (\"ngram_stop\", CountVectorizer(\n",
    "        stop_words=stopwords, ngram_range=(1,2), min_df=2\n",
    "    )),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('compl_nb-weight_bal', ComplementNB()),\n",
    "])\n",
    "\n",
    "res = run_pipes(\n",
    "    [pipe0, pipe1, pipe2, pipe3, pipe4], splits=10, corpus=corpus, target=y\n",
    ")\n",
    "print_table(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "intense-experience",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-21T22:11:46.074715Z",
     "start_time": "2022-11-21T22:11:46.016659Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename = \"topic_classifier.pickle\"\n",
    "# Save\n",
    "pickle.dump(pipe0, open(filename, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"topic_classifier.pickle\"\n",
    "model = pickle.load(open(filename, 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(corpus)\n",
    "score = model.score(corpus, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict([\"What is your name ?\"])\n",
    "print(label_encoder.inverse_transform(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "universal-extension",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_topic = list(label_encoder.inverse_transform([0, 1, 2, 3, 4, 5, 6]))\n",
    "liste_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = model.predict_proba([\"usb C\"])\n",
    "\n",
    "liste_proba = list(pred_proba[0])\n",
    "liste_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-variety",
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_topic = list(label_encoder.inverse_transform([0, 1, 2, 3, 4, 5, 6]))\n",
    "pred_proba = model.predict_proba([\"usb C\"])\n",
    "liste_proba = list(pred_proba[0])\n",
    "rg = len(liste_proba)\n",
    "for i in range(7):\n",
    "    print(f'topic {i+1} :',liste_topic[np.argmax(liste_proba)])\n",
    "    idx = np.argmax(liste_proba)\n",
    "    print(idx)\n",
    "    liste_proba.pop(idx)\n",
    "    liste_topic.pop(idx)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_proba = {liste_topic[i]: liste_proba[i] for i in range(len(liste_proba))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "horizontal-belgium",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = metrics.confusion_matrix(y, y_pred)\n",
    "\n",
    "plt.figure(figsize=(9,9))\n",
    "sns.heatmap(cm, annot=True, fmt=\".3f\", linewidths=.5, square = True, cmap = 'Blues_r');\n",
    "plt.yticks(np.arange(len(topics))+0.5, tuple(label_encoder.inverse_transform([0, 1, 2, 3, 4, 5, 6])), rotation=0)\n",
    "plt.ylabel('Actual label');\n",
    "plt.xticks(np.arange(len(topics))+0.5, tuple(label_encoder.inverse_transform([0, 1, 2, 3, 4, 5, 6])), rotation=60)\n",
    "plt.xlabel('Predicted label');\n",
    "all_sample_title = 'Accuracy Score: {0} %'.format(round(score,3)*100)\n",
    "plt.title(all_sample_title, size = 15);\n",
    "print(label_encoder.inverse_transform([0, 1, 2, 3, 4, 5, 6]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-detection",
   "metadata": {},
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pipelinehelper import PipelineHelper\n",
    "#\n",
    "#pipe5 = Pipeline([\n",
    "#    ('rbg2grey', RGB2GrayTransformer()),\n",
    "#    ('hog', HogTransformer(pixels_per_cell=(14, 14), cells_per_block=(2, 2), \n",
    "#                           orientations=9, block_norm='L2-Hys')\n",
    "#    ),\n",
    "#    ('scaler', PipelineHelper([\n",
    "#        ('std', StandardScaler()),\n",
    "#        ('max', MaxAbsScaler()),\n",
    "#    ])),\n",
    "#    ('classifier', PipelineHelper([\n",
    "#        ('sgd', SGDClassifier(random_state=42, max_iter=1000, tol=1e-3)),\n",
    "#        ('svm', SVC()),\n",
    "#        ('gnb', GaussianNB()),\n",
    "#        ('cnb', ComplementNB()),\n",
    "#    ])),\n",
    "#])\n",
    "\n",
    "pipe5 = Pipeline([\n",
    "    (\"vect\", CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('sgd', SGDClassifier()),\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "invalid-printer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#params = {\n",
    "#    'hog__orientations': [8, 9],\n",
    "#    'hog__cells_per_block': [(2, 2), (3, 3), (4, 4)],\n",
    "#    'hog__pixels_per_cell': [(8, 8), (10, 10), (12, 12)],\n",
    "#    'scaler__selected_model': pipe.named_steps['scaler'].generate({\n",
    "#        'std__with_mean': [True, False],\n",
    "#        'std__with_std': [True, False],\n",
    "#        'max__copy': [True],\n",
    "#    }),\n",
    "#    'classifier__selected_model': pipe.named_steps['classifier'].generate({\n",
    "#        'sgd__loss': ['hinge', 'log', 'modified_huber', 'perceptron', 'squared_loss', 'huber'],\n",
    "#        'svm__C': [0.1, 1.0],\n",
    "#        'svm__kernel': ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']\n",
    "#    })\n",
    "#}\n",
    "\n",
    "params ={\n",
    "    'vect__stop_words': [None, 'english'], ## stopvords\n",
    "    'vect__ngram_range': [(1,2), (1,3)], ## ngrams\n",
    "    'vect__min_df': [2],\n",
    "    #'tfidf_use_idf': [True, False],\n",
    "    'sgd__alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1e0], ## learning rate\n",
    "    'sgd__max_iter':[1000], ## nb of epochs\n",
    "    'sgd__average': ['micro', 'macro', 'weighted'],\n",
    "    'sgd__class_weight': ['micro', 'macro', 'balanced'], ## macro\n",
    "    'sgd__loss': ['hinge', 'log', 'modified_huber', 'squared_hinge', 'perceptron', \n",
    "                  'squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'], ## model choice SVM, logistic reg...\n",
    "    'sgd__penalty':['l2', 'l1', 'elasticnet']\n",
    "}\n",
    "\n",
    "## affichage des paramètre \n",
    "\n",
    "SGDClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-questionnaire",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params ={\n",
    "    'vect__stop_words': ['english'], ## stopvords\n",
    "    'vect__ngram_range': [(1,2)], ## ngrams\n",
    "    'vect__min_df': [2],\n",
    "    #'tfidf_use_idf': [True, False],\n",
    "    'sgd__alpha': [ 1e-4, 1e-3], ## learning rate\n",
    "    'sgd__max_iter':[1000], ## nb of epochs\n",
    "    'sgd__average': ['micro', 'macro', 'weighted'],\n",
    "    'sgd__class_weight': ['micro', 'macro', 'balanced'], ## macro\n",
    "    'sgd__loss': ['hinge', 'modified_huber','squared_epsilon_insensitive'], ## model choice SVM, logistic reg...\n",
    "    'sgd__penalty':['elasticnet']\n",
    "}\n",
    "\n",
    "#f1_scorer = make_scorer(f1_score , average='macro')# Ne marche pas, utiliser onevsrestClassif ou onevsone\n",
    "gs = GridSearchCV(pipe5, \n",
    "                  params, \n",
    "                  cv=3,\n",
    "                  n_jobs=-1,\n",
    "                  scoring='accuracy',\n",
    "                  verbose=1,\n",
    "                  return_train_score=True)\n",
    " \n",
    "gs_res = gs.fit(corpus, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-refund",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('meilleur score obtenu :', gs_res.best_score_)\n",
    "print('meilleur estimateur :', gs_res.best_estimator_)\n",
    "print('meilleur paramètres :', gs_res.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funky-proposition",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "discord-smartbot",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "notify_time": "10",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Table of Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "306.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": "20",
    "lenType": "20",
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
